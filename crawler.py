"""
ë‚˜ë¼ì¥í„° ì…ì°°ê³µê³  í¬ë¡¤ëŸ¬
ì›¹ ìŠ¤í¬ë˜í•‘ì„ í†µí•´ ì…ì°° ê³µê³  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  DBì— ì €ì¥
"""

import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
from typing import List, Dict, Optional
import logging

from config import settings
from database import SessionLocal
from models import Bidding


# ===== ë¡œê¹… ì„¤ì • =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
# ë¡œê·¸ ì˜ˆì‹œ: 2025-01-23 10:30:45 - INFO - í¬ë¡¤ë§ ì‹œì‘


class NaramarketCrawler:
    """
    ë‚˜ë¼ì¥í„° í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
    
    ê¸°ëŠ¥:
    - ì…ì°°ê³µê³  ëª©ë¡ í¬ë¡¤ë§
    - ìƒì„¸ ì •ë³´ ì¶”ì¶œ
    - DB ì €ì¥
    """
    
    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.base_url = "http://www.g2b.go.kr:8101/ep/tbid/tbidList.do"
        self.session = requests.Session()  # ì„¸ì…˜ ì¬ì‚¬ìš© (ì†ë„ í–¥ìƒ)
        
        # User-Agent ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        logger.info("âœ… ë‚˜ë¼ì¥í„° í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    
    def get_bidding_list(self, page: int = 1) -> Optional[BeautifulSoup]:
        """
        ì…ì°°ê³µê³  ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            page: í˜ì´ì§€ ë²ˆí˜¸
            
        Returns:
            BeautifulSoup: íŒŒì‹±ëœ HTML ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        params = {
            'area': '00',              # ì „ì²´ ì§€ì—­
            'searchType': '1',          # ê²€ìƒ‰ ìœ í˜•
            'bidNm': '',                # ê²€ìƒ‰ì–´ (ì—†ìŒ = ì „ì²´)
            'currentPageNo': page       # í˜ì´ì§€ ë²ˆí˜¸
        }
        
        try:
            logger.info(f"ğŸ“„ {page}í˜ì´ì§€ ìš”ì²­ ì¤‘...")
            
            response = self.session.get(
                self.base_url,
                params=params,
                headers=self.headers,
                timeout=10  # 10ì´ˆ ì•ˆì— ì‘ë‹µ ì—†ìœ¼ë©´ íƒ€ì„ì•„ì›ƒ
            )
            
            # ì‘ë‹µ ìƒíƒœ í™•ì¸
            response.raise_for_status()  # 4xx, 5xx ì—ëŸ¬ ì‹œ ì˜ˆì™¸ ë°œìƒ
            
            # ì¸ì½”ë”© ì„¤ì • (í•œê¸€ ê¹¨ì§ ë°©ì§€)
            response.encoding = 'utf-8'
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            logger.info(f"âœ… {page}í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
            return soup
            
        except requests.exceptions.Timeout:
            logger.error(f"âŒ {page}í˜ì´ì§€ íƒ€ì„ì•„ì›ƒ")
            return None
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ {page}í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
    
    
    def parse_bidding_item(self, item) -> Optional[Dict]:
        """
        ì…ì°°ê³µê³  í•­ëª© íŒŒì‹± (ë‚˜ë¼ì¥í„° ì‹¤ì œ HTML êµ¬ì¡° ë°˜ì˜)
        
        Args:
            item: BeautifulSoup íƒœê·¸ ê°ì²´ (ê³µê³  1ê°œ <tr> íƒœê·¸)
            
        Returns:
            dict: íŒŒì‹±ëœ ë°ì´í„° ë˜ëŠ” None
        """
        try:
            data = {}
            
            # ëª¨ë“  td íƒœê·¸ ê°€ì ¸ì˜¤ê¸°
            tds = item.find_all('td', class_='w2group_w2tb_td')
            
            if len(tds) < 5:
                logger.warning("âš ï¸ td íƒœê·¸ ë¶€ì¡± - ê±´ë„ˆëœ€")
                return None
            
            # data-title ì†ì„±ìœ¼ë¡œ í•„ë“œ ì‹ë³„
            for td in tds:
                title = td.get('data-title', '')
                text = td.text.strip()
                
                # ê³µê³ ì¢…ë¥˜
                if 'ê³µê³ ì¢…ë¥˜' in title:
                    data['contract_method'] = text
                
                # ì…ì°°ë°©ì‹
                elif 'ì…ì°°ë°©ì‹' in title:
                    data['bidding_method'] = text
                
                # ì…ì°°ë°©ë²•
                elif 'ì…ì°°ë°©ë²•' in title:
                    pass  # í•„ìš”ì‹œ ì¶”ê°€
            
            # ê³µê³ ë²ˆí˜¸ ì¶”ì¶œ (label íƒœê·¸ ì—†ëŠ” tdì—ì„œ)
            notice_td = None
            for td in item.find_all('td'):
                if 'data-title' not in td.attrs and td.text.strip().startswith('R'):
                    notice_td = td
                    break
            
            if notice_td:
                data['notice_number'] = notice_td.text.strip()
            else:
                logger.warning("âš ï¸ ê³µê³ ë²ˆí˜¸ ì—†ìŒ - ê±´ë„ˆëœ€")
                return None
            
            # ê³µê³ ëª… ì¶”ì¶œ (label íƒœê·¸ ì•ˆì—)
            title_label = item.find('label')
            if title_label:
                data['title'] = title_label.text.strip()
            else:
                logger.warning("âš ï¸ ê³µê³ ëª… ì—†ìŒ - ê±´ë„ˆëœ€")
                return None
            
            # ë°œì£¼ê¸°ê´€ / ìˆ˜ìš”ê¸°ê´€ ì¶”ì¶œ
            # ì‹¤ì œ êµ¬ì¡°ì—ì„œ ì •í™•í•œ ìœ„ì¹˜ í™•ì¸ í•„ìš” (ì„ì‹œë¡œ None)
            data['ordering_agency'] = None
            data['demanding_agency'] = None
            
            # ì˜ˆì‚°ê¸ˆì•¡ / ì¶”ì •ê°€ê²© (ì‹¤ì œ í•„ë“œëª… í™•ì¸ í•„ìš”)
            data['budget_amount'] = None
            data['estimated_price'] = None
            
            # ê³µê³ ì¼ì‹œ / ì…ì°°ë§ˆê°ì¼ì‹œ
            # ë‚ ì§œ í˜•ì‹: "2025/10/23 (2025/10/..." ê°™ì€ íŒ¨í„´
            date_pattern = r'(\d{4}/\d{2}/\d{2})'
            for td in tds:
                text = td.text.strip()
                if '/' in text and len(text) > 8:
                    import re
                    match = re.search(date_pattern, text)
                    if match:
                        try:
                            date_str = match.group(1)
                            parsed_date = datetime.strptime(date_str, '%Y/%m/%d')
                            
                            # ì²« ë²ˆì§¸ ë‚ ì§œ = ê³µê³ ì¼ì‹œ
                            if data.get('notice_date') is None:
                                data['notice_date'] = parsed_date
                            # ë‘ ë²ˆì§¸ ë‚ ì§œ = ì…ì°°ë§ˆê°ì¼ì‹œ
                            elif data.get('bid_close_date') is None:
                                data['bid_close_date'] = parsed_date
                        except ValueError:
                            pass
            
            # URL ìƒì„± (ìƒì„¸ í˜ì´ì§€ ë§í¬)
            # ì‹¤ì œ ë§í¬ êµ¬ì¡° í™•ì¸ í•„ìš”
            data['bidding_url'] = None
            link_tag = item.find('a')
            if link_tag and link_tag.get('href'):
                href = link_tag['href']
                if href.startswith('http'):
                    data['bidding_url'] = href
                else:
                    data['bidding_url'] = f"http://www.g2b.go.kr{href}"
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if not data.get('notice_number') or not data.get('title'):
                logger.warning("âš ï¸ í•„ìˆ˜ í•„ë“œ ëˆ„ë½ - ê±´ë„ˆëœ€")
                return None
            
            return data
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì‹± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    
    def save_to_db(self, data: Dict) -> bool:
        """
        ë°ì´í„°ë¥¼ DBì— ì €ì¥
        
        Args:
            data: ì…ì°°ê³µê³  ë°ì´í„°
            
        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        db = SessionLocal()
        
        try:
            # ì¤‘ë³µ í™•ì¸ (ê°™ì€ ê³µê³ ë²ˆí˜¸ê°€ ì´ë¯¸ ìˆëŠ”ì§€)
            existing = db.query(Bidding).filter(
                Bidding.notice_number == data['notice_number']
            ).first()
            
            if existing:
                logger.info(f"â­ï¸ ì¤‘ë³µ ê³µê³  ê±´ë„ˆëœ€: {data['notice_number']}")
                return False
            
            # ìƒˆ ê³µê³  ê°ì²´ ìƒì„±
            bidding = Bidding(
                notice_number=data.get('notice_number'),
                title=data.get('title'),
                ordering_agency=data.get('ordering_agency'),
                budget_amount=data.get('budget_amount'),
                notice_date=data.get('notice_date'),
                bidding_url=data.get('bidding_url')
            )
            
            # DBì— ì¶”ê°€
            db.add(bidding)
            db.commit()
            
            logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {data['notice_number']} - {data['title'][:30]}")
            return True
            
        except Exception as e:
            db.rollback()  # ì—ëŸ¬ ë°œìƒ ì‹œ ë¡¤ë°±
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
            
        finally:
            db.close()
    
    
    def crawl(self, max_pages: int = None) -> Dict[str, int]:
        """
        í¬ë¡¤ë§ ì‹¤í–‰ (ë©”ì¸ í•¨ìˆ˜)
        
        Args:
            max_pages: ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜ (Noneì´ë©´ ì„¤ì •ê°’ ì‚¬ìš©)
            
        Returns:
            dict: í¬ë¡¤ë§ ê²°ê³¼ í†µê³„
        """
        if max_pages is None:
            max_pages = settings.MAX_PAGES
        
        logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘ - ìµœëŒ€ {max_pages}í˜ì´ì§€")
        
        stats = {
            'total_items': 0,    # ì „ì²´ í•­ëª© ìˆ˜
            'saved_items': 0,     # ì €ì¥ ì„±ê³µ
            'skipped_items': 0,   # ì¤‘ë³µìœ¼ë¡œ ê±´ë„ˆëœ€
            'failed_items': 0     # ì‹¤íŒ¨
        }
        
        for page in range(1, max_pages + 1):
            # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            soup = self.get_bidding_list(page)
            
            if soup is None:
                logger.warning(f"âš ï¸ {page}í˜ì´ì§€ ê±´ë„ˆëœ€")
                continue
            
            # ê³µê³  ëª©ë¡ ì°¾ê¸°
            # ì‹¤ì œ HTML: <tr id="mf_wfm_container_..." class="w2group_up">
            items = soup.find_all('tr', class_='w2group_up')
            
            # ë˜ëŠ” tbody ë‚´ì˜ ëª¨ë“  tr ì°¾ê¸° (idê°€ mf_wfmìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒë§Œ)
            if not items:
                tbody = soup.find('tbody')
                if tbody:
                    items = [tr for tr in tbody.find_all('tr') 
                            if tr.get('id', '').startswith('mf_wfm')]
            
            if not items:
                logger.warning(f"âš ï¸ {page}í˜ì´ì§€ì— ê³µê³  ì—†ìŒ")
                break
            
            logger.info(f"ğŸ“‹ {page}í˜ì´ì§€: {len(items)}ê°œ ê³µê³  ë°œê²¬")
            
            # ê° ê³µê³  ì²˜ë¦¬
            for item in items:
                stats['total_items'] += 1
                
                # íŒŒì‹±
                data = self.parse_bidding_item(item)
                
                if data is None:
                    stats['failed_items'] += 1
                    continue
                
                # ì €ì¥
                saved = self.save_to_db(data)
                
                if saved:
                    stats['saved_items'] += 1
                else:
                    stats['skipped_items'] += 1
            
            # ì„œë²„ ë¶€ë‹´ ì¤„ì´ê¸° (ìš”ì²­ ê°„ ëŒ€ê¸°)
            time.sleep(settings.CRAWL_DELAY)
        
        logger.info(f"""
ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!
ğŸ“Š ê²°ê³¼:
   - ì „ì²´: {stats['total_items']}ê°œ
   - ì €ì¥: {stats['saved_items']}ê°œ
   - ì¤‘ë³µ: {stats['skipped_items']}ê°œ
   - ì‹¤íŒ¨: {stats['failed_items']}ê°œ
        """)
        
        return stats


# ===== ì‹¤í–‰ ì½”ë“œ =====
if __name__ == "__main__":
    """
    ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í–ˆì„ ë•Œë§Œ ë™ì‘
    
    ì‹¤í–‰ ë°©ë²•:
    python crawler.py
    """
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = NaramarketCrawler()
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    results = crawler.crawl()
    
    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ: {results['saved_items']}ê°œ ì €ì¥ë¨")


"""
ğŸ’¡ í¬ë¡¤ë§ ë™ì‘ íë¦„:

1. NaramarketCrawler ê°ì²´ ìƒì„±
   â†“
2. crawl() ë©”ì„œë“œ í˜¸ì¶œ
   â†“
3. í˜ì´ì§€ 1~10 ë°˜ë³µ:
   - get_bidding_list(page) â†’ HTML ê°€ì ¸ì˜¤ê¸°
   - ê³µê³  ëª©ë¡ ì°¾ê¸°
   - ê° ê³µê³ ë§ˆë‹¤:
     * parse_bidding_item() â†’ ë°ì´í„° ì¶”ì¶œ
     * save_to_db() â†’ DB ì €ì¥
   - time.sleep() â†’ ëŒ€ê¸°
   â†“
4. í†µê³„ ì¶œë ¥
"""


"""
ğŸ’¡ ì—ëŸ¬ ì²˜ë¦¬:

1. íƒ€ì„ì•„ì›ƒ â†’ í•´ë‹¹ í˜ì´ì§€ ê±´ë„ˆëœ€
2. íŒŒì‹± ì‹¤íŒ¨ â†’ í•´ë‹¹ ê³µê³  ê±´ë„ˆëœ€
3. ì¤‘ë³µ â†’ ì €ì¥ ì•ˆí•˜ê³  ê±´ë„ˆëœ€
4. DB ì €ì¥ ì‹¤íŒ¨ â†’ rollback í›„ ê±´ë„ˆëœ€

â†’ ì¼ë¶€ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰!
"""


"""
ğŸ’¡ ì‹¤ì œ ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­:

ì´ ì½”ë“œëŠ” "í…œí”Œë¦¿"ì…ë‹ˆë‹¤!
ë‚˜ë¼ì¥í„°ì˜ ì‹¤ì œ HTML êµ¬ì¡°ë¥¼ í™•ì¸í•˜ê³ :

1. CSS ì„ íƒì ìˆ˜ì •:
   item.find('td', class_='number')
   â†’ ì‹¤ì œ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ë³€ê²½

2. í•„ë“œ ì¶”ê°€:
   - demanding_agency (ìˆ˜ìš”ê¸°ê´€)
   - contract_method (ê³„ì•½ë°©ë²•)
   - bidding_method (ì…ì°°ë°©ë²•)
   ë“± ì¶”ê°€

3. ë‚ ì§œ í˜•ì‹ í™•ì¸:
   '%Y-%m-%d %H:%M'
   â†’ ì‹¤ì œ í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •
"""
